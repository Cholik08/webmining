{"cells":[{"cell_type":"markdown","source":["**Crawling data** merupakan sebuah teknik pengumpulan data dimana data tidak ditulis secara manual melainkan menggunakan mesin. Salah satu library yang dapat digunakan untuk melakukan crawling adalah dengan menggunakan twint. twint merupakan libraby yang dapat digunakan untuk melakukan crawling data pada twitter. adapun persiapan yang harus dilakukan terlebih dahulu adalah menginstall jupyter-book sebagai tools untuk membuat konten."],"metadata":{"id":"Chp0Yyel_Qbq"}},{"cell_type":"markdown","source":["Pertama-tama masuk ke folder webmining terlebih dahulu"],"metadata":{"id":"RGxT3gBshhBS"}},{"cell_type":"markdown","source":["Kemudian install twin dari github dengan code seperti berikut:"],"metadata":{"id":"xAWK31kG_XkA"}},{"cell_type":"code","source":["!pip install git+https://github.com/twintproject/twint.git"],"metadata":{"id":"3AZvCbBy0yyv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Setelah itu install juga nest asynco"],"metadata":{"id":"0Y_i7Td-_fhE"}},{"cell_type":"code","source":["!pip install nest_asyncio"],"metadata":{"id":"5sqNyUUk02vi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import twint\n","import nest_asyncio\n","nest_asyncio.apply() #digunakan sekali untuk mengaktifkan tindakan serentak dalam notebook jupyter."],"metadata":{"id":"5aOfB08H2WLK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dalam melakukan crawl menggunakan **twint** ada beberapa baris code yang dituliskan. Dibawah ini merupakan code untuk melakukan crawl menggunakan twint:"],"metadata":{"id":"7sOtt95Z__HA"}},{"cell_type":"code","source":["import twint\n","c = twint.Config()\n","\n","c.Since = '2022-09-18'\n","c.Until = '2022-09-19'\n","c.Search = 'ospek'\n","c.Output = 'dataOspek.csv'\n","\n","twint.run.Search(c)"],"metadata":{"id":"D6zXTa622YXe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Setelah data dicrawling selanjutnya dilakukan pra preprocessing atau bisa juga disebut **data cleaning**. Apa itu **data cleaning** ? data sudah bersih yang dimana pada data tersebut tidak terdapat url, emoji, backslice dan yang lainnya. Tujuan dari data cleaning yaitu agar data yang masuk kedalam mesin itu murni berupa data teks sehingga mesin lebih mudah dalam melakukan pembelajaran."],"metadata":{"id":"fW56tiy0ltGM"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","data_tweet = pd.read_csv('dataospektweet.csv') #membaca file csv dengan nama output\n","data_tweet.head()"],"metadata":{"id":"eYB-5vWA2c0q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Selanjutnya adalah mengganti data index 1 secara manual untuk menghapus emoji karena emoji tersebut tidak valid sehingga tidak bisa dihapus menggunakan mesin"],"metadata":{"id":"rHI7MX3CAOnQ"}},{"cell_type":"code","source":["data_tweet[\"tweet\"][1] = \"Cuz i don't have a proper weekend...   Terus nyadar kalau minggu depan jg ada ospek offline\"\n","data_tweet[\"tweet\"][1]"],"metadata":{"id":"FzRNHOj02gqG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Case Folding**"],"metadata":{"id":"oDwO3sF0AY3o"}},{"cell_type":"code","source":["data_tweet[\"tweet\"] = data_tweet[\"tweet\"].str.lower()\n","data_tweet[\"tweet\"]"],"metadata":{"id":"dvxU3zo-2tOx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Cleaning Data**"],"metadata":{"id":"dSYjXFokmzxn"}},{"cell_type":"code","source":["import nltk\n","import string\n","import re\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","from nltk.probability import FreqDist"],"metadata":{"id":"t9SY_NmN2wMK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Yang pertama adalah menghapus url"],"metadata":{"id":"RKt4vE2Nm5Pa"}},{"cell_type":"code","source":["def remove_url(text):\n","  return re.sub(r'http\\S+', '', text)\n","data_tweet[\"tweet\"] = data_tweet[\"tweet\"].apply(remove_url)"],"metadata":{"id":"Zx5rcRoa2zi3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Selanjutnya adalah menghapus karekter spesial"],"metadata":{"id":"DDCX_8GXnC4E"}},{"cell_type":"code","source":["def remove_tweet_special(text):\n","  #remove tab, new line, and back slice\n","  text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n","  #remove non ASCII (emoticon,chinese word,.etc)\n","  text = text.encode('ascii','replace').decode('ascii')\n","  #remove mention, link, hastag\n","  text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\s+)\", \" \", text).split())\n","  #remove incomplete url\n","  return text.replace(\"http://\",\" \").replace(\"https://\",\" \")\n","data_tweet[\"tweet\"] = data_tweet[\"tweet\"].apply(remove_tweet_special)"],"metadata":{"id":"X9zsyJsd23O0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def remove_number(text):\n","  return re.sub(r\"\\d+\", \"\",text)\n","data_tweet[\"tweet\"] = data_tweet[\"tweet\"].apply(remove_number)"],"metadata":{"id":"H0z3kREm26sd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def remove_punctuation(text):\n","  return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n","data_tweet[\"tweet\"] = data_tweet[\"tweet\"].apply(remove_punctuation)"],"metadata":{"id":"9jVH9GBg29wc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def remove_whitespace(text):\n","  return text.strip()\n","data_tweet[\"tweet\"] = data_tweet[\"tweet\"].apply(remove_whitespace)"],"metadata":{"id":"a2aBbzf43BzF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def remove_singl_char(text):\n","  return re.sub(r\"\\b[a-zA-Z]\\b\",\"\",text)\n","data_tweet[\"tweet\"] = data_tweet[\"tweet\"].apply(remove_singl_char)"],"metadata":{"id":"sZvktseY3FNJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def word_tokenize_wrapper(text):\n","  return word_tokenize(text)\n","data_tweet[\"tweet_tokens\"] = data_tweet[\"tweet\"].apply(word_tokenize_wrapper)"],"metadata":{"id":"kPhXNe3O3HsO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_tweet[\"tweet\"].head()"],"metadata":{"id":"yhcUeRno3TR1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_tweet[\"tweet_tokens\"].head()"],"metadata":{"id":"4GWQmPnp3WZi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def freqDist_wrapper(text):\n","  return FreqDist(text)\n","\n","data_tweet[\"tweet_tokens_fdist\"] = data_tweet[\"tweet_tokens\"].apply(freqDist_wrapper)\n","data_tweet[\"tweet_tokens_fdist\"].head()"],"metadata":{"id":"yY3f6oaI3Xrh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Stopwrord Removal**"],"metadata":{"id":"HsR3W-xIn7md"}},{"cell_type":"code","source":["#import nltk untuk melakukan peprocessing data\n","import nltk\n","nltk.download('stopwords') #download stopword indonesia\n","from nltk.corpus import stopwords\n","stopword_language = stopwords.words('indonesian') #set stopword\n","stopword_language.extend(['jg','jos','yg','dg','dgn','rt','ny','d','klo','kalo',\n","                          'amp','biar','bikin','bilang','gak','ga','krn','nya','nih',\n","                          'sih','si','tau','tdk','tuh','utk','ya','jd','jgn','sdh','aja',\n","                          'n','t',\n","                          ])\n","txt_stopword = pd.read_csv('dataospektweet.csv',names=['stopwords'],header=None)\n","stopword_language.extend(txt_stopword['stopwords'][0].split(' '))"],"metadata":{"id":"P98Ejf_z3c3i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def stopword_removal(words):\n","  return [word for word in words if word not in stopword_language]\n","\n","data_tweet[\"tweet_tokens_rstopword\"] = data_tweet[\"tweet_tokens\"].apply(stopword_removal)"],"metadata":{"id":"YbmY2_953frb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Normalisasi**"],"metadata":{"id":"PP9-J-NCoSFI"}},{"cell_type":"code","source":["normalized_word = pd.read_csv('dataospektweet.csv')\n","normalized_word_dict = {}\n","\n","for index,row in normalized_word_dict:\n","  if row[0] not in normalized_word_dict:\n","    normalized_word_dict[row[0]]=row[1]\n","\n","def normalized_term(document):\n","  return [normalized_word_dict[term] if term in normalized_word_dict else term for term in document]\n","\n","data_tweet[\"tweet_normalized\"] = data_tweet[\"tweet_tokens_rstopword\"].apply(normalized_term)\n","data_tweet[\"tweet_normalized\"].head()"],"metadata":{"id":"lnbm07lG3klr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Stemming**"],"metadata":{"id":"juDdZp9qAtMX"}},{"cell_type":"code","source":["!pip install sastrawi"],"metadata":{"id":"vnrcUi9r1GiF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install swifter"],"metadata":{"id":"Y50ehH_o1I5I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n","import swifter\n","\n","factory = StemmerFactory()\n","stemmer = factory.create_stemmer()\n","\n","def stemmed_wrapper(term):\n","  return stemmer.stem(term)\n","\n","term_dict = {}\n","\n","for document in data_tweet['tweet_normalized']:\n","  for term in document:\n","    if term not in term_dict:\n","      term_dict[term] = ' '\n","\n","print(len(term_dict))\n","print(\"------------------------\")\n","\n","for term in term_dict:\n","  term_dict[term] = stemmed_wrapper(term)\n","  print(term,':',term_dict[term])\n","\n","print(term_dict)\n","print(\"------------------------\")"],"metadata":{"id":"g125m6Mn3obB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_stemmed_term(document):\n","  return [term_dict[term] for term in document]\n","\n","data_tweet[\"tweet_tokens_stemmed\"] = data_tweet[\"tweet_normalized\"].swifter.apply(get_stemmed_term)"],"metadata":{"id":"xgjA1Sk83rnp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Membuat VSM**"],"metadata":{"id":"NkhxoyMrpiuC"}},{"cell_type":"code","source":["data_tweet.to_csv(\"preprocessing_ospek.csv\")"],"metadata":{"id":"bErmTx1Z3uuX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install sklearn"],"metadata":{"id":"sdWoYwTo1UCb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","count = CountVectorizer()\n","titles = pd.read_csv('preprocessing_ospek_sample.csv',sep=',',usecols=[\"tweet_tokens_stemmed\"],squeeze=True)\n","\n","docs = titles.values\n","bag = count.fit_transform(docs)\n","print(docs)"],"metadata":{"id":"JQJynuIy3xt-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(count.vocabulary_)"],"metadata":{"id":"Scn6KJxi30V2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(count.get_feature_names())\n","a = count.get_feature_names()"],"metadata":{"id":"FVLZUa_I35CJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(bag.toarray())\n","b = bag.toarray()"],"metadata":{"id":"_xFcQg6z38-d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dfa = pd.DataFrame(data=a)\n","num_rows = -1\n","\n","for row in open(\"preprocessing_ospek_sample.csv\"):\n","  num_rows+=1\n","\n","dfb = pd.DataFrame(data=b,index=range(0,num_rows),columns=[a])\n","dfb"],"metadata":{"id":"kAFATpct3_Tk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dflabels = pd.read_csv('labels.csv')\n","dfb['labels'] = dflabels #menambahkan kolom labels dari file labels.csv\n","dfb"],"metadata":{"id":"qxYUrU9b4CnG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#import pembagian train dan test dari sklearn model selection\n","from sklearn.model_selection import train_test_split\n","#import mutual information untuk klasifikasi\n","from sklearn.feature_selection import mutual_info_classif\n","#set data training dan data testing dari data\n","X_train,X_test,y_train,y_test=train_test_split(dfb.drop(labels=['labels'], axis=1),\n","    dfb['labels'],\n","    test_size=0.3,\n","    random_state=0)\n","mutual_info = mutual_info_classif(X_train, y_train)\n","mutual_info"],"metadata":{"id":"-n_uuyjl4FJE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Sort**"],"metadata":{"id":"J1P13D8HqveK"}},{"cell_type":"code","source":["mutual_info = pd.Series(mutual_info)\n","mutual_info.index = X_train.columns\n","mutual_info.sort_values(ascending=False)"],"metadata":{"id":"D8bE1ytI4I5K"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}